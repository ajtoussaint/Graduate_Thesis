{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f845585f-c0ab-4400-bca7-eced13331fcd",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71945bb2-a390-4016-9533-c9bbcdc72564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static path variables\n",
    "paths = {\n",
    "    \"ASSIST09\" : \"data/ASSISTments2009/\",\n",
    "    \"NEUR20\" : \"data/NeurIPS2020/\",\n",
    "}\n",
    "wranglers = {\n",
    "    \"ASSIST09\" : \"data/ASSISTments2009/assistments09_wrangler\", \n",
    "    \"NEUR20\" : \"data/NeurIPS2020/neurIPS2020_wrangler\",\n",
    "}\n",
    "models = { #might not need this\n",
    "    \"IRT\" : \"models/IRT/IRT\",\n",
    "    \"NCDM\": \"models/NCDM/NCDM\"\n",
    "}\n",
    "prepper_path = \"data/dataPrepper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05eb094-1f93-4528-ba69-74d9688035f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def checkExists(path):\n",
    "    #remove the .py file if it exists\n",
    "    if(os.path.exists(path)):\n",
    "        print(\"found \" + path)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"couldn't find \" + path)\n",
    "        print(f\"Try running: !jupyter nbconvert --to script {path[:-2] + 'ipynb'}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ffc54f-9820-4d95-aca4-2d434ff08923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found data/dataPrepper.py\n",
      "found models/IRT/IRT.py\n",
      "found models/NCDM/NCDM.py\n",
      "found data/ASSISTments2009/assistments09_wrangler.py\n",
      "found data/NeurIPS2020/neurIPS2020_wrangler.py\n"
     ]
    }
   ],
   "source": [
    "# A temporary measure while working in notebooks\n",
    "# Ensure all necesary scripts have been converted from notebooks\n",
    "allExists = True\n",
    "\n",
    "allExists = checkExists(prepper_path + \".py\") and allExists\n",
    "\n",
    "for key in models.keys():\n",
    "    allExists = checkExists(models[key] + \".py\") and allExists\n",
    "\n",
    "for key in wranglers.keys():\n",
    "    allExists = checkExists(wranglers[key] + \".py\") and allExists\n",
    "\n",
    "if(not allExists):\n",
    "    print(\"\\nMissing critical .py file(s), see above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52eeae8-0519-4ff3-8335-0e963c11383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script data/dataPrepper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83579f0-09cb-4402-a446-50f222c5935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select a model to test:\n",
      "1.IRT\n",
      "2.NCDM\n",
      " 2\n",
      "Select a dataset to test on:\n",
      "1.ASSIST09\n",
      "2.NEUR20\n",
      " 1\n",
      "Select a run condition:\n",
      "1.basic\n",
      "2.sampled\n",
      "3.correctSaturated\n",
      "4.incorrectSaturated\n",
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing test run of NCDM using ASSIST09 in basic format\n"
     ]
    }
   ],
   "source": [
    "modelList ={\n",
    "    \"1\":\"IRT\",\n",
    "    \"2\":\"NCDM\"\n",
    "}\n",
    "\n",
    "datasetList = {\n",
    "    \"1\":\"ASSIST09\",\n",
    "    \"2\":\"NEUR20\",\n",
    "}\n",
    "\n",
    "runTypeList = {\n",
    "    \"1\":\"basic\",\n",
    "    \"2\":\"sampled\",\n",
    "    \"3\":\"correctSaturated\",\n",
    "    \"4\":\"incorrectSaturated\",\n",
    "}\n",
    "\n",
    "model = modelList[input(f\"Select a model to test:\\n\" + \"\\n\".join([f'{k}.{modelList[k]}' for k in modelList.keys()]) + \"\\n\")]\n",
    "dataset = datasetList[input(f\"Select a dataset to test on:\\n\" + \"\\n\".join([f'{k}.{datasetList[k]}' for k in datasetList.keys()]) + \"\\n\")]\n",
    "runType = runTypeList[input(f\"Select a run condition:\\n\" + \"\\n\".join([f'{k}.{runTypeList[k]}' for k in runTypeList.keys()]) + \"\\n\")]\n",
    "\n",
    "print(f\"Performing test run of {model} using {dataset} in {runType} format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b7c878-bb5d-408b-9a93-aaab784e8548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing split from ASSIST09 to train NCDM for basic format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading existing data:: 100%|████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from data import dataPrepper as prep\n",
    "\n",
    "Q, data = prep.prepareData(dataset = dataset, runType = runType, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c197f79d-0efe-45a5-9635-1619b0b8e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.IRT import IRT\n",
    "from models.NCDM import NCDM\n",
    "modelFuncs = {\n",
    "    \"IRT\": IRT.run_IRT,\n",
    "    \"NCDM\":NCDM.run_NCDM,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee03231-644c-4c9b-8317-ff8f101619ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCDM detects 4286 users, 6650 items, and 381 knowledge concepts\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2784",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     26\u001b[0m     start_timer \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 27\u001b[0m     acc, auc, mae, rmse \u001b[38;5;241m=\u001b[39m \u001b[43mmodelFuncs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     end_timer \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     30\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "File \u001b[1;32m~\\Documents\\AJT WKU\\Thesis\\Graduate_Thesis\\Testing\\models\\NCDM\\NCDM.py:210\u001b[0m, in \u001b[0;36mrun_NCDM\u001b[1;34m(Q, train_data, test_data, valid_data)\u001b[0m\n\u001b[0;32m    201\u001b[0m     data_set \u001b[38;5;241m=\u001b[39m TensorDataset(\n\u001b[0;32m    202\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(user, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# (1, user_n) to (0, user_n-1)\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(item, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# (1, item_n) to (0, item_n-1)\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         knowledge_emb,\n\u001b[0;32m    205\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(score, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(data_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 210\u001b[0m train_set, valid_set, test_set \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    211\u001b[0m     transform(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], item2knowledge, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m [train_data, valid_data, test_data]\n\u001b[0;32m    213\u001b[0m ]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m#check for GPU and select or run on cpu\u001b[39;00m\n\u001b[0;32m    216\u001b[0m device_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\AJT WKU\\Thesis\\Graduate_Thesis\\Testing\\models\\NCDM\\NCDM.py:211\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    201\u001b[0m     data_set \u001b[38;5;241m=\u001b[39m TensorDataset(\n\u001b[0;32m    202\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(user, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# (1, user_n) to (0, user_n-1)\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(item, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# (1, item_n) to (0, item_n-1)\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         knowledge_emb,\n\u001b[0;32m    205\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(score, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(data_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    210\u001b[0m train_set, valid_set, test_set \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 211\u001b[0m     \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem2knowledge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m [train_data, valid_data, test_data]\n\u001b[0;32m    213\u001b[0m ]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m#check for GPU and select or run on cpu\u001b[39;00m\n\u001b[0;32m    216\u001b[0m device_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\AJT WKU\\Thesis\\Graduate_Thesis\\Testing\\models\\NCDM\\NCDM.py:199\u001b[0m, in \u001b[0;36mrun_NCDM.<locals>.transform\u001b[1;34m(user, item, item2knowledge, score, batch_size)\u001b[0m\n\u001b[0;32m    197\u001b[0m knowledge_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(item), knowledge_n))\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(item)):\n\u001b[1;32m--> 199\u001b[0m     knowledge_emb[idx][np\u001b[38;5;241m.\u001b[39marray(\u001b[43mitem2knowledge\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    201\u001b[0m data_set \u001b[38;5;241m=\u001b[39m TensorDataset(\n\u001b[0;32m    202\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(user, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# (1, user_n) to (0, user_n-1)\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(item, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# (1, item_n) to (0, item_n-1)\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     knowledge_emb,\n\u001b[0;32m    205\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(score, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    206\u001b[0m )\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(data_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2784"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "#find average training correct/incorrect ratio per student (should be ~0.50)\n",
    "avg_train_score = []\n",
    "avg_test_score = []\n",
    "for d in data:\n",
    "    avg_train_score.append(d[\"train\"].groupby('user_id')[\"score\"].mean().mean())\n",
    "    avg_test_score.append(d[\"test\"].groupby('user_id')[\"score\"].mean().mean())\n",
    "\n",
    "result_object = {\n",
    "    \"model\" : model,\n",
    "    \"runType\" : runType,\n",
    "    \"dataset\" : dataset,\n",
    "    \"test_correct_ratio\":np.mean(avg_test_score),\n",
    "    \"train_correct_ratio\":np.mean(avg_train_score),\n",
    "}\n",
    "\n",
    "accs, aucs, maes, rmses, times = [], [], [], [], []\n",
    "\n",
    "#run each provided data configuration and collect statistics\n",
    "\n",
    "for run in data:\n",
    "    start_timer = time.time()\n",
    "    acc, auc, mae, rmse = modelFuncs[model](Q, run[\"train\"], run[\"test\"], run[\"valid\"])\n",
    "    end_timer = time.time()\n",
    "    \n",
    "    accs.append(acc)\n",
    "    aucs.append(auc)\n",
    "    maes.append(mae)\n",
    "    rmses.append(rmse)\n",
    "    times.append(end_timer - start_timer)\n",
    "\n",
    "\n",
    "#format the output\n",
    "result_object['ACC'] = np.mean(accs)\n",
    "result_object['AUC'] = np.mean(aucs)\n",
    "result_object['MAE'] = np.mean(maes)\n",
    "result_object['RMSE'] = np.mean(rmses)\n",
    "result_object['ACC_std'] = np.std(accs)\n",
    "result_object['AUC_std'] = np.std(aucs)\n",
    "result_object['MAE_std'] = np.std(maes)\n",
    "result_object['RMSE_std'] = np.std(rmses)\n",
    "result_object['avg_train_duration'] = np.mean(times)\n",
    "result_object['performed_at'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0847fd5-d88a-4948-b9ed-a9bfa0c9ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#print the output to a csv\n",
    "results_path = \"results.csv\"\n",
    "\n",
    "#if the csv doesn't exist create it\n",
    "if not os.path.exists(results_path):\n",
    "    df = pd.DataFrame([result_object])\n",
    "else:\n",
    "    #pull in the csv as a dataframe\n",
    "    df = pd.read_csv(results_path)\n",
    "    #remove any existing row that has the same...\n",
    "        #model, runtype, and dataset\n",
    "    df = df[~((df['model'] == model) & (df['runType'] == runType) & (df['dataset'] == dataset))]\n",
    "    #add the new row\n",
    "    df = pd.concat([df, pd.DataFrame(result_object, index=[1])], ignore_index=True)\n",
    "df.to_csv(results_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89506368-83bb-42c7-a7ac-e301004464c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
