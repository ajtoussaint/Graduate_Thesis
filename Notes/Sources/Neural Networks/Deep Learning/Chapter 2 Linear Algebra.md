- Linear algebra is continuous mathematics
## 2.1 Matrix Vector Scalar
- [[Scalar]] is a number represented in lower case italics 
- [[Vector]] is an ordered array represented in lower case bold
- [[Matrix]] is a 2D array represented in uppercase bold
- $\mathbb{R}^n$ represents the cartesian product of $\mathbb{R}$ n times over
- The [[Transpose]] of a matrix converts the columns to rows and incidentally vice versa
- Matrix addition is simple just add the values in place, matrices must be same shape
- [[Broadcasting]] is when a vector or scalar is assumed to be copied when added to a matrix so that the shapes can be compatible
## 2.2 Multiplying Matrices
- Matrix multiplication is more complex, to achieve AB A must have a column for each row in b. If a is m x n and B is n x p then the result of multiplication will be a m x p matrix C where $C_{i,j} = \sum_{k=0}^{n} A_{i,k} B_{k,j}$  
	- Matrix multiplication is associative, distributive, but NOT commutative
- An element wise product is also defined for a Matrix by using an approach similar to addition
- A [[Dot Product]] is $x \cdot y = xy^T$ for vectors x and y. has the commutative property
- A [[System of Linear Equations]] in the context of linear algebra can be defined as $Ax = b$ where A is a known matrix, b is a known vector, and x is an unknown vector
	- A system has either 0, 1, or infinitely many solutions
## 2.3 Identity and Inverse
- The [[Identity Matrix]] is a matrix with all zeros except for scalar values along the diagonal
	- Any vector multiplied by the identity matrix will be unchanged
- The [[Inverse Matrix]] defined as $A^{-1}$  and $A^{-1}A=I_n$ 
	- This property is used to solve a [[System of Linear Equations]] $x = A^{-1}b$ 
	- $A^{-1}$ is not typically used in computers practically speaking
## 2.4 Linear Dependence and Span
- A [[Linear Combination]] is a set of vectors multiplied by a corresponding scalar and summed $\sum_i c_{i}v^{(i)}$ 
- The [[Span]] of a set of vectors is the set of all points obtainable by [[Linear Combination]] of the vectors
-  (Delete this line)
- The [[Range]] of a  [[Matrix]] is the [[Span]] of the columns of that matrix.
- To solve a [[System of Linear Equations]] is to evaluate what [[Vector]] b is in the [[Range]] of [[Matrix]] A
- For a [[System of Linear Equations]] to have a solution for all $b \in \mathbb{R}^{m}$ the [[Range]] of A must be all $\mathbb{R}^{m}$ and therefore must have at least m columns
- [[Linear Dependence]] a set of [[Vector|vectors]] within a [[Matrix]] are linearly dependent if they are redundant
- A [[Matrix]] has [[Linear Independence]] when no columns are redundant
- For A to encompass  $\mathbb{R}^{m}$ it must have **exactly** m [[Linear Dependence|linearly independent]] columns
- For A to have an [[Inverse Matrix]] to itself $Ax=b$ must have at **most** one solution, therefore A must have at most m columns
## 2.5 Norms
- A [[Square]] [[Matrix]] has the same number of rows as columns
- A [[Singular]] [[Matrix]] is a [[Square]] [[Matrix]] that also has [[Linear Independence]] 
- A [[System of Linear Equations]] can only be solved by inversion if A is [[Square]] and **not** [[Singular]]
- A [[Norm]] is a function to measure vector size
	- In general an $L^{p}$ norm is a norm defined by $\lVert x \rVert _p = (\sum_i \lvert x \rvert ^p)^{1/p}$ 
	- However, it can be any function where $f(0) = 0$, $f(x + y) \le f(x) + f(y)$ , and $\forall \alpha \in \mathbb{R}, f(\alpha x ) = \lvert \alpha \rvert f(x)$ 
	- The Max Norm L$\infty$ Norm is simply the absolute value of the largest element in a [[Vector]] 
- The [[Euclidian Norm]] is a [[Norm]] where $p=2$   which is often simply $\lVert x \rVert$ 
	- The derivatives of the [[Euclidian Norm]] depend on the entire vector
	- The Euclidian Norm increases slowly near the origin which is undesirable when it is important to distinguish very small values from zero, in this case use the L1 [[Norm]] which grows at a constant rate
- The [[Squared L2 Norm]] is equivalent to the [[Euclidian Norm]] squared  $\lVert x \rVert^2 = x^{T}x$ 
	- The derivative of the [[Squared L2 Norm]] depends only on $x_i$ not the entire vector
- The [[Frobenius Norm]] is aa [[Euclidian Norm]] for a [[Matrix]] : $\lVert A \rVert = \sqrt{\sum_{i,j}A_{i,j}^{2}}$ 
## 2.6 Special Matrices and Vectors
- A [[Diagonal]] [[Matrix]] has the property that $D_{ij} = 0$ if $i \ne j$ 
	- The [[Identity Matrix]] is [[Diagonal]]
	- It is efficient to multiply: $diag(v)x = v \odot x$ (element wise multiplication)
		- The diag operation [[Broadcasting|broadcasts]] a [[vector]] of length n into a n x n diagonal [[Matrix]]
	- If every diagonal element in the matrix is non-zero then: $diag(V)^{-1} = diag([1/v_1 ... 1/v_n]^T)$ 
- A [[Symmetric]] [[Matrix]] is when $A = A^T$ 
	- This often occurs when a matrix is generated by an algorithm with two arguments and the order of the arguments doesn't affect the result
- A [[Unit]] [[Vector]] is a vector where $\lVert x \rVert = 1$ 
- A [[Vector]] is [[Orthogonal]] to another vector when $x^Ty = 0$ 
- A [[Matrix]] is [[Orthogonal]] in general when $A^{-1} = A^T$ 
	- By definition an orthogonal matrix has orthogonal and [[Normal]] rows (orthonormal)
## 2.7 Eigendecomposition
- An [[Eigenvector]] is a [[Vector]] corresponding to some [[Matrix]] that, when multiplied by its matrix, maintains the ratio between its components but is scaled
	- $Av = \lambda v$ ($v$ is the [[Eigenvector]] and $\lambda$ is an [[Eigenvalue]]) 
- A [[Matrix]] undergoes [[Eigendecomposition]] by the following equation: $A = Vdiag(\lambda)V^{-1}$ 
	- V is a matrix where each column is an [[Eigenvector]] of A
	- Sometimes this only works with use of complex numbers
	- Every real [[Symmetric]] [[Matrix]] can be decomposed into this form of expression using only real [[Eigenvector|eigenvectors]] and [[Eigenvalue|eigenvalues]]: $A = Q\Lambda Q^{T}$ 
		- Q is an [[Orthogonal]] [[Matrix]] of [[Eigenvector|eigenvectors]] and $\Lambda$ is a [[Diagonal]] [[Matrix]] of [[Eigenvalue|eigenvalues]] where the value in column i and row i represents the [[Eigenvalue]] of column i of Q
		- It is convention to sort $\Lambda$ by ascending [[Eigenvalue|eigenvalues]] so that the [[Eigendecomposition]] of a real [[Symmetric]] [[Matrix]] is only unique if the [[Eigenvalue|eigenvalues]] of the matrix are unique
- A [[Matrix]] is [[Singular]] iff its [[Eigenvalue|eigenvalues]] are 0
- A [[Positive]] [[Matrix]] includes only positive values including 0
- A [[Negative]] [[Matrix]] includes only negative values including 0
- A [[Definite]] [[Matrix]] includes no 0s
	- For a [[Positive]] [[Definite]] [[Matrix]] $x^TAx = 0 \therefore x=0$ 
- A [[Semidefinite]] [[Matrix]] includes some 0s
	- For a [[Positive]] [[Semidefinite]] [[Matrix]] $\forall x, x^TAx\ge0$ 
## 2.8 Singular Value Decomposition
- [[Singular Value Decomposition]] is a form that can be applied more generally than [[Eigendecomposition]] 
	- every real [[Matrix]] has a [[Singular Value Decomposition]]: $A=UDV^T$  
	- A is an m x n matrix, U is m x m , and V is n x n, D is n x m
	- U and V are [[Orthogonal]] [[Matrix|matrices]]  and D is a [[Diagonal]] [[Matrix]] but not necessarily [[Square]]
	- D's diagonal includes the [[Singular Values]], 
	- U's Columns include the Left [[Singular Vectors]] and V's columns include the Right [[Singular Vectors]]
- [[Moore-Penrose Pseudoinverse]]: $A^{+} = \lim_{\alpha \to 0} (A^{T}A+\alpha I)^{-1}A^{T}$ so $A^{+} = VD^{+}U^{T}$ 
	- Used for inverses of Rectangular [[Matrix|Matrices]] 
	- $D^+$ is obtained from D by taking the reciprocal of its nonzero elements and then takin the [[Transpose]] of the resulting [[Matrix]]
	- $x = A^{+}y$ when A has more columns then rows such that x is a solution with the minimum [[Euclidian Norm]] $\lVert x \rVert_2$ among all possible solutions
	- If A has more rows than columns may be no solution, but we get the closest possible solution based on $\lVert Ax-y \rVert_2$ 
## 2.10 Trace Operator
- The [[Trace Operator]] is the sum of all [[Diagonal]] entries in a [[Matrix]] $Tr(A) = \sum_{i}A_{i,i}$ 
	- [[Frobenius Norm]] $\lVert A \rVert_F = \sqrt{T_r(AA^T)}$ 
	- [[Transpose]] $Tr(A) = Tr(A^T)$ 
	- $Tr(ABC) = Tr(CAB) = Tr(BCA)$ (only move last matrix to the front)
		- $\therefore Tr(\Pi_{i=1}^{n}F^{(i)}) = Tr(F^{(n)} \Pi_{i=1}^{n-1}F_i)$
	- $a = Tr(a)$ for [[Scalar]]
## 2.11 Determinant
- A [[Determinant]] maps a [[Matrix]] to a real [[Scalar]] 
	- It is the product of all the [[Eigenvalue|Eigenvalues]] $det(A) = \Pi_i \lambda_i$ 
	- Measures how much the matrix multiplies the size of space
## 2.12 Principal Components Analysis
- [[Principal Component Analysis]] is used to encode/decode [[Vector|Vectors]] into a compressed space while minimizing loss of information
	- encode $x$ using $f(x) = c$ and decode with $g(f(x)) \approx x$ 
	- To encode from $\mathbb{R}^{n} \to \mathbb{R}^l$ use $g(c) = Dc$ where $D \in \mathbb{R}^{n \times l}$ 
	- D is $n \times l$ has columns [[Orthogonal]] to each other and each column has a [[Unit]] [[Norm]]
	- Optimal code $c^* = argmin \lVert x - g(c) \rVert$ so we select each coded point to minimize [[Euclidian Norm]] or [[Squared L2 Norm]] for simplicity 
	- Evaluating this minimum yields: $c = D^Tx$ which is minimized using vector calculus as described in [[Chapter 4 Numerical Computation#4.3 Gradient-Based Optimization|4.3]] selecting a D that minimizes [[Frobenius Norm]]
- [[Lossy Compression]] means information is stored with less memory at the cost of losing some information

[[Goodfellow, Bengio, Courville 2016]]